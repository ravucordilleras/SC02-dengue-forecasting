{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import main packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Specific Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from logitboost import LogitBoost\n",
    "\n",
    "from tqdm import tqdm\n",
    "# model selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import confusion_matrix, auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifiers, and corresponding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierNames = pd.DataFrame([\"kNN\", \"DT\", \"AdaBoost\",\"GNB\",\"LR\", \"LDA\",\"SVM.SVC\",\"LogitBoost\"])\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial'),\n",
    "    # LinearDiscriminantAnalysis(solver='lsqr', shrinkage=0.81),  # rv.kris\n",
    "    LinearDiscriminantAnalysis(),  # rv.kris\n",
    "    make_pipeline(StandardScaler(), SVC(gamma='auto')),\n",
    "    LogitBoost(n_estimators=200, random_state=0)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define settings for printing, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data from file\n",
    "Data contains 3 columns: 'date', 'rain1'(1-day cumulative rainfall), and 'ls' (boolean to indicate if date is a landslide or non-landslide event). Rainfall data, from 2014 to 2018 is from PAGASA Baguio synoptic station, whereas landslide or non-landslide event is determined from the maintenance records of the DPWH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv file, get 'date', 'rain', and 'landslide' columns\n",
    "rainls = pd.read_csv(\"DATAhumidity.csv\", usecols=list(range(0, 12)))\n",
    "\n",
    "# format date column\n",
    "rainls[\"date\"] = pd.to_datetime(rainls[\"date\"])\n",
    "\n",
    "# show first five rows of dataframe\n",
    "print(rainls.head(10))\n",
    "\n",
    "# show dimensions of the dataframe (rows, columns)\n",
    "rainls.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features\n",
    "\n",
    "With the hypothesis that the landslide occurrences are determined by certain rainfall characteristics, we create features (columns) from the rainfall time series in our input data.  We explore combinations of cumulative and offset functions to generate the various rainfall characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL FEATURE GENERATION\n",
    "shift = list(range(0, 175))     #(25 weeks) #(Carvajal, T.M., Viacrusis, K.M., Hernandez, L.F.T. et al. Machine learning methods reveal the temporal pattern of dengue incidence using meteorological factors in metropolitan Manila, Philippines. BMC Infect Dis 18, 183 (2018). https://doi.org/10.1186/s12879-018-3066-0)\n",
    "shiftrain = pd.DataFrame(index=rainls.index)\n",
    "for i in shift:\n",
    "    shiftrain[f\"shift_{str(i)}\"] = rainls[\"rain1\"].shift(-i)\n",
    "    shiftrain[f\"shiftmmaxh_{str(i)}\"] = rainls[\"meanmaxh\"].shift(-i)\n",
    "    shiftrain[f\"shiftmminh_{str(i)}\"] = rainls[\"meanminh\"].shift(-i)\n",
    "    shiftrain[f\"shiftmeanh_{str(i)}\"] = rainls[\"meanh\"].shift(-i)\n",
    "    shiftrain[f\"shiftrh_{str(i)}\"] = rainls[\"relhumh\"].shift(-i)\n",
    "# adding lables to samples (rows)\n",
    "shiftrain[\"den\"] = rainls.den\n",
    "# removing NaNs as result of cumulative and lag functions\n",
    "shiftrain = shiftrain.dropna(axis=0, how=\"any\")\n",
    "COrain = shiftrain\n",
    "COrain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting, training, testing, evaluating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEAN MAX MINIMUM TEMPERATURE RELATIVE HUMIDITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = list(range(1))        # rv.kris - this increases the number of iterations, set this to 100, but this also multiples the training time by the same amount\n",
    "\n",
    "# define split function\n",
    "n_splits = 30\n",
    "test_size = 0.25\n",
    "random_state = None             # rv.kris - place your seed here, author used \"0\" (zero)\n",
    "sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# create dataframes for evaluation results\n",
    "TPRs = pd.DataFrame(index=range(n_splits), columns=classifierNames[0])\n",
    "FPRs = pd.DataFrame(index=range(n_splits), columns=classifierNames[0])\n",
    "TNRs = pd.DataFrame(index=range(n_splits), columns=classifierNames[0])\n",
    "aROCs = pd.DataFrame(index=range(n_splits), columns=classifierNames[0])\n",
    "\n",
    "# loop through all splits\n",
    "with tqdm(total=len(my_list)) as pbar:\n",
    "    for x in my_list:\n",
    "        for s, (train_index, test_index) in zip(TPRs.index, sss.split(COrain.iloc[:, :-1], COrain.iloc[:, -1])):\n",
    "            # define training and testing sets using results of the splitting function\n",
    "            ytrain, ytest = COrain[\"den\"].iloc[train_index], COrain[\"den\"].iloc[test_index]\n",
    "            Xtrain, Xtest = COrain[COrain.columns[:-1]].iloc[train_index], COrain[COrain.columns[:-1]].iloc[test_index]\n",
    "            # normalize training set features (column-wise)\n",
    "            Xtrain_norm, Xnorm = normalize(Xtrain, axis=0, return_norm=True)\n",
    "            # normalize testing set features (column-wise)\n",
    "            Xtest_norm = Xtest/Xnorm\n",
    "            # loop through algorithms\n",
    "            for c in range(len(classifierNames)):\n",
    "                # if c in [0, 1, 2, 3, 4, 6, 7, 8]:continue      # skip some algorithms # rv.kris-  the defaults used by the author was 1, 8... we can set this to all except 5 (LDA)...\n",
    "                if c in [1, 8]:continue      # skip some algorithms\n",
    "                # print(x, s, c, classifiers[c])\n",
    "                # define current classifier\n",
    "                clf = classifiers[c]\n",
    "                # train the classifier using the training set\n",
    "                clf.fit(Xtrain_norm, ytrain)\n",
    "                # predict values using the trained classifier and the test set\n",
    "                Xtest_norm = np.array(Xtest_norm)         # rv.kris\n",
    "                y_pred = clf.predict(Xtest_norm)\n",
    "                # Evaluate the test\n",
    "                # Define true values\n",
    "                y_true = ytest\n",
    "                # Produce confusion matrix from true and predicted values\n",
    "                tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "                # Compute true positive and false positive rates and assign to dataframe\n",
    "                tpr = np.round((tp / (tp+fn*1.)), 4)\n",
    "                fpr = np.round((fp / (fp+tn*1.)), 4)\n",
    "                tnr = np.round((tn / (tn+fp*1.)), 4)\n",
    "                err = np.round((fp+fn / (tn+fp+tp+fn*1.)), 4)\n",
    "                aROC = auc([0, fpr, 1], [0, tpr, 1])\n",
    "                # Assign results to dataframes\n",
    "                TPRs.loc[s, classifierNames.values[c][0]] = tpr\n",
    "                TNRs.loc[s, classifierNames.values[c][0]] = tnr\n",
    "                FPRs.loc[s, classifierNames.values[c][0]] = fpr\n",
    "                # ERRs.loc[s, classifierNames.values[c][0]] = err\n",
    "                aROCs.loc[s, classifierNames.values[c][0]] = aROC\n",
    "            pbar.update(1)\n",
    "\n",
    "# Remove NA columns (skipped algorithms)\n",
    "FPRs = FPRs.dropna(axis=1, how=\"all\")\n",
    "TPRs = TPRs.dropna(axis=1, how=\"all\")\n",
    "TNRs = TNRs.dropna(axis=1, how=\"all\")\n",
    "# ERRs = ERRs.dropna(axis=1, how=\"all\")\n",
    "aROCs = aROCs.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_list = list(range(100))\n",
    "# convert results to percent (0-100)\n",
    "_FPRs = FPRs * 100\n",
    "_TPRs = TPRs * 100\n",
    "_TNRs = TNRs * 100\n",
    "# ERRs = ERRs * 100\n",
    "_aROCs = aROCs * 100\n",
    "\n",
    "print(_aROCs)\n",
    "\n",
    "# _aROCs[\"kNN\"] = _aROCs[\"kNN\"].astype(float)                       # rv.kris\n",
    "# _aROCs[\"AdaBoost\"] = _aROCs[\"AdaBoost\"].astype(float)             # rv.kris\n",
    "# _aROCs[\"GNB\"] = _aROCs[\"GNB\"].astype(float)                       # rv.kris\n",
    "# _aROCs[\"LR\"] = _aROCs[\"LR\"].astype(float)                         # rv.kris\n",
    "_aROCs[\"LDA\"] = _aROCs[\"LDA\"].astype(float)                       # rv.kris\n",
    "# _aROCs[\"SVM.SVC\"] = _aROCs[\"SVM.SVC\"].astype(float)               # rv.kris\n",
    "# _aROCs[\"LogitBoost\"] = _aROCs[\"LogitBoost\"].astype(float)         # rv.kris\n",
    "# _TPRs[\"kNN\"] = _TPRs[\"kNN\"].astype(float)                         # rv.kris\n",
    "# _TPRs[\"AdaBoost\"] = _TPRs[\"AdaBoost\"].astype(float)               # rv.kris\n",
    "# _TPRs[\"GNB\"] = _TPRs[\"GNB\"].astype(float)                         # rv.kris\n",
    "# _TPRs[\"LR\"] = _TPRs[\"LR\"].astype(float)                           # rv.kris\n",
    "_TPRs[\"LDA\"] = _TPRs[\"LDA\"].astype(float)                         # rv.kris\n",
    "# _TPRs[\"SVM.SVC\"] = _TPRs[\"SVM.SVC\"].astype(float)                 # rv.kris\n",
    "# _TPRs[\"LogitBoost\"] = _TPRs[\"LogitBoost\"].astype(float)           # rv.kris\n",
    "# _FPRs[\"kNN\"] = _FPRs[\"kNN\"].astype(float)                         # rv.kris\n",
    "# _FPRs[\"AdaBoost\"] = _FPRs[\"AdaBoost\"].astype(float)               # rv.kris\n",
    "# _FPRs[\"GNB\"] = _FPRs[\"GNB\"].astype(float)                         # rv.kris\n",
    "# _FPRs[\"LR\"] = _FPRs[\"LR\"].astype(float)                           # rv.kris\n",
    "_FPRs[\"LDA\"] = _FPRs[\"LDA\"].astype(float)                         # rv.kris\n",
    "# _FPRs[\"SVM.SVC\"] = _FPRs[\"SVM.SVC\"].astype(float)                 # rv.kris\n",
    "# _FPRs[\"LogitBoost\"] = _FPRs[\"LogitBoost\"].astype(float)           # rv.kris\n",
    "# _TNRs[\"kNN\"] = _TNRs[\"kNN\"].astype(float)                         # rv.kris\n",
    "# _TNRs[\"AdaBoost\"] = _TNRs[\"AdaBoost\"].astype(float)               # rv.kris\n",
    "# _TNRs[\"GNB\"] = _TNRs[\"GNB\"].astype(float)                         # rv.kris\n",
    "# _TNRs[\"LR\"] = _TNRs[\"LR\"].astype(float)                           # rv.kris\n",
    "_TNRs[\"LDA\"] = _TNRs[\"LDA\"].astype(float)                         # rv.kris\n",
    "# _TNRs[\"SVM.SVC\"] = _TNRs[\"SVM.SVC\"].astype(float)                 # rv.kris\n",
    "# _TNRs[\"LogitBoost\"] = _TNRs[\"LogitBoost\"].astype(float)           # rv.kris\n",
    "\n",
    "# HISTOGRAMS\n",
    "# Define parameters for histograms\n",
    "bins = 10\n",
    "stacked = True\n",
    "figsize = (6, 4)\n",
    "# Plot histograms\n",
    "ax1 = _aROCs.plot.hist(stacked=stacked, bins=bins, figsize=figsize)\n",
    "ax1.legend(ncol=2)\n",
    "ax1.set_xlabel(\"aROC, %\", fontsize=\"large\")\n",
    "plt.savefig(\"aROCshistmeanmaxminrel.png\")\n",
    "\n",
    "ax2 = _TPRs.plot.hist(stacked=stacked, bins=bins, figsize=figsize)\n",
    "ax2.legend(ncol=2)\n",
    "ax2.set_xlabel(\"TPR, %\", fontsize=\"large\")\n",
    "plt.savefig(\"TPRshistmeanmaxminrel.png\")\n",
    "\n",
    "ax3 = _FPRs.plot.hist(stacked=stacked, bins=bins, figsize=figsize)\n",
    "ax3.legend(ncol=2)\n",
    "ax3.set_xlabel(\"FPR, %\", fontsize=\"large\")\n",
    "plt.savefig(\"FPRshistmeanmaxminrel.png\")\n",
    "\n",
    "ax7 = _TNRs.plot.hist(stacked=stacked, bins=bins, figsize=figsize)\n",
    "ax7.legend(ncol=2)\n",
    "ax7.set_xlabel(\"TNR, %\", fontsize=\"large\")\n",
    "plt.savefig(\"TNRshistmeanmaxminrel.png\")\n",
    "\n",
    "# SCATTER PLOT\n",
    "# Define figure parameters\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax4 = fig.add_subplot(111)\n",
    "marker = \"o\"\n",
    "markersize = 10\n",
    "linewidth = 0\n",
    "alpha = 0.5\n",
    "mfc = \"None\"\n",
    "mew = 2\n",
    "# Loop through results per algorithm\n",
    "with tqdm(total=len(my_list)) as pbar:\n",
    "    for cN in _FPRs.columns:\n",
    "        # plot results of current algorithm\n",
    "        ax4.plot(_FPRs[cN].values, _TPRs[cN].values, marker=marker, mfc=mfc, mew=mew, markersize=markersize, linewidth=linewidth, alpha=alpha, label=cN)\n",
    "        # Add random guess line\n",
    "    pbar.update(1)\n",
    "\n",
    "ax4.plot([0, 100], [0, 100], \"k:\", label=\"random guess\")\n",
    "# Format plot\n",
    "ax4.legend()\n",
    "ax4.set_ylabel(\"TPR, %\", fontsize=\"large\")\n",
    "ax4.set_xlabel(\"FPR, %\", fontsize=\"large\")\n",
    "ax4.set_xlim(-5, 105)\n",
    "ax4.set_ylim(-5, 105)\n",
    "plt.savefig(\"ROCplotMLmeanmaxminrel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABULAR RESULTS\n",
    "aROCsum = pd.DataFrame()\n",
    "aROCsum[\"mean\"] = _aROCs.mean()\n",
    "aROCsum[\"std\"] = _aROCs.std()\n",
    "aROCsum[\"min\"] = _aROCs.min()\n",
    "aROCsum[\"maxn\"] = _aROCs.max()\n",
    "\n",
    "TPRsum = pd.DataFrame()\n",
    "TPRsum[\"mean\"] = _TPRs.mean()\n",
    "TPRsum[\"std\"] = _TPRs.std()\n",
    "TPRsum[\"min\"] = _TPRs.min()\n",
    "TPRsum[\"maxn\"] = _TPRs.max()\n",
    "\n",
    "FPRsum = pd.DataFrame()\n",
    "FPRsum[\"mean\"] = _FPRs.mean()\n",
    "FPRsum[\"std\"] = _FPRs.std()\n",
    "FPRsum[\"min\"] = _FPRs.min()\n",
    "FPRsum[\"maxn\"] = _FPRs.max()\n",
    "\n",
    "TNRsum = pd.DataFrame()\n",
    "TNRsum[\"mean\"] = _TNRs.mean()\n",
    "TNRsum[\"std\"] = _TNRs.std()\n",
    "TNRsum[\"min\"] = _TNRs.min()\n",
    "TNRsum[\"maxn\"] = _TNRs.max()\n",
    "\n",
    "display(TNRsum)\n",
    "display(FPRsum)\n",
    "display(TPRsum)\n",
    "display(aROCsum)\n",
    "\n",
    "# revert result to percent (0.0 - 1.0)\n",
    "# FPRs = FPRs/100\n",
    "# TPRs = TPRs/100\n",
    "# aROCs = aROCs/100\n",
    "# TNRs = TNRs/100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with traditional cumulative rainfall thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crain = pd.DataFrame(index=rainls.index)\n",
    "\n",
    "cmltv = [0, 175, 42, 49, 84]\n",
    "for C in cmltv:\n",
    "    Crain[f\"c{str(C)}\"] = rainls[\"rain1\"].shift(-C)\n",
    "\n",
    "# addding labels to samples (rows)\n",
    "Crain[\"den\"] = rainls.den\n",
    "#removing NaNs as result of cumulative and lag functions\n",
    "Crain = Crain.dropna(axis=0, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import scoreatpercentile\n",
    "# define percentiles\n",
    "minpct = 1\n",
    "maxpct = 99\n",
    "steps = 100\n",
    "rainpct = np.linspace(minpct, maxpct, steps)\n",
    "# define dataframes for storing thresholds and score-at-percentiles\n",
    "thresholds = pd.DataFrame(index=rainpct, columns=Crain.columns[:-1])\n",
    "tprs = pd.DataFrame(index=rainpct, columns=Crain.columns[:-1])\n",
    "fprs = pd.DataFrame(index=rainpct, columns=Crain.columns[:-1])\n",
    "aucs = pd.DataFrame(index=rainpct, columns=Crain.columns[:-1])\n",
    "# SCATTER PLOT\n",
    "# define figure parameters\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot results of ML algorithms\n",
    "marker = \"o\"\n",
    "markersize = 10\n",
    "linewidth = 0\n",
    "mfc = \"None\"\n",
    "mew = 2\n",
    "# alpha = 0.5\n",
    "alpha = 0.7\n",
    "# convert results to percent (0-100)\n",
    "for cN in _FPRs.columns:\n",
    "    # plot results of current algorithm\n",
    "    ax.plot(_FPRs[cN].values, _TPRs[cN].values, marker=marker, mew=mew, mfc=mfc, markersize=markersize, linewidth=linewidth, alpha=alpha, label=cN)\n",
    "    \n",
    "alpha = 1\n",
    "lw = 1\n",
    "\n",
    "# Loop through cumulative rainfalls\n",
    "for c in Crain.columns[:-1]:\n",
    "    # compute score-at-percentiles for the current cumulative rainfall\n",
    "    thresholds[c] = scoreatpercentile(Crain[c].values, thresholds.index)\n",
    "    \n",
    "    # loop through thresholds for the current cumulative rainfall\n",
    "    for t in thresholds.index:\n",
    "        # evaluate rainfall exceedence of the current threshold\n",
    "        predict = np.where(Crain[c].values >= thresholds.loc[t, c], 1, 0)\n",
    "        # evaluate predictive performance\n",
    "        tn, fp, fn, tp = confusion_matrix(Crain[\"den\"].values, predict).ravel()\n",
    "        tpr = tp / (tp+fn)\n",
    "        fpr = fp / (fp+tn)\n",
    "        #store results to dataframes\n",
    "        tprs.loc[t, c] = tpr\n",
    "        fprs.loc[t, c] = fpr\n",
    "        aucs.loc[t, c] = auc([0, fpr, 1], [0, tpr, 1])\n",
    "        \n",
    "    ax.plot(100 * fprs.loc[:, c], 100 * tprs.loc[:, c], lw=lw, label=f\"{c} {str(int(100 * aucs.loc[:, c].max()))}\", alpha=alpha)\n",
    "    \n",
    "#add random guess line\n",
    "ax.plot([0, 100], [0, 100], \"k:\", label=\"random guess\")\n",
    "\n",
    "# format plot\n",
    "ax.legend(loc=\"lower right\", ncol=2)\n",
    "ax.set_ylabel(\"TPR, %\", fontsize=\"large\")\n",
    "ax.set_xlabel(\"FPR, %\", fontsize=\"large\")\n",
    "ax.set_xlim(-5, 105)\n",
    "ax.set_ylim(-5, 105)\n",
    "plt.savefig(\"ROCplotML+cmltvmeanmaxminrel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABULAR RESULTS\n",
    "aucsum = pd.DataFrame()\n",
    "aucsum[\"mean\"] = aucs.mean()\n",
    "aucsum[\"std\"] = aucs.std()\n",
    "aucsum[\"min\"] = aucs.min()\n",
    "aucsum[\"maxn\"] = aucs.max()\n",
    "\n",
    "tprsum = pd.DataFrame()\n",
    "tprsum[\"mean\"] = tprs.mean()\n",
    "tprsum[\"std\"] = tprs.std()\n",
    "tprsum[\"min\"] = tprs.min()\n",
    "tprsum[\"maxn\"] = tprs.max()\n",
    "\n",
    "fprsum = pd.DataFrame()\n",
    "fprsum[\"mean\"] = fprs.mean()\n",
    "fprsum[\"std\"] = fprs.std()\n",
    "fprsum[\"min\"] = fprs.min()\n",
    "fprsum[\"maxn\"] = fprs.max()\n",
    "\n",
    "print(\"False Positives:\")\n",
    "print(fprsum)\n",
    "print()\n",
    "print(\"True Positives:\")\n",
    "print(tprsum)\n",
    "print()\n",
    "print(\"Aucsum:\")\n",
    "print(aucsum)\n",
    "\n",
    "# revert results to percent (0.0 - 1.0)\n",
    "fprs = fprs/100\n",
    "tprs = tprs/100\n",
    "aucs = aucs/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define percentiles\n",
    "minpct = 1\n",
    "maxpct = 99\n",
    "steps = 30\n",
    "rainpct = np.linspace(minpct, maxpct, steps)\n",
    "# define dataframes for storing thresholds and score-at percentiles\n",
    "tprs = pd.DataFrame(index=rainpct,columns=Crain.columns[:-1])\n",
    "fprs = pd.DataFrame(index=rainpct,columns=Crain.columns[:-1])\n",
    "aucs = pd.DataFrame(index=rainpct,columns=Crain.columns[:-1])\n",
    "# SCATTER PLOT\n",
    "# define figure parameters\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot results of ML algorithms\n",
    "marker = \"o\"\n",
    "markersize = 10\n",
    "linewidth = 0\n",
    "mfc = \"None\"\n",
    "mew = 2\n",
    "alpha = 0.7\n",
    "\n",
    "for cN in _FPRs.columns:\n",
    "    ax.plot(_FPRs[cN].values, _TPRs[cN].values, marker=marker, mew=mew, mfc=mfc, markersize=markersize, linewidth=linewidth, alpha=alpha, label=cN)\n",
    "\n",
    "alpha = 1\n",
    "lw = 1\n",
    "\n",
    "# add random guess line\n",
    "ax.plot([0, 100], [0, 100], \"k:\", label=\"random guess\")\n",
    "\n",
    "# format plot\n",
    "ax.legend(loc=\"lower right\", ncol=2)\n",
    "ax.set_ylabel(\"TPR, %\", fontsize=\"large\")\n",
    "ax.set_xlabel(\"FPR, %\", fontsize=\"large\")\n",
    "ax.set_xlim(-5, 105)\n",
    "ax.set_ylim(-5, 105)\n",
    "plt.savefig(\"ROCplotML.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
